{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Description: \n",
    "Performing stochastic gradient descent to find a global function on P1 that satisfies given differentiability conditions (analogous to a finding metric which satisfies Ricci-flatness in tutorial 3)...\n",
    "P1 is built from C2(z0,z1), and hence is parameterised by these homogeneous coordinates.\n",
    "The differentiability conditions our desired function, F (modelled by f), satisifes are:\n",
    "    1) \\frac{\\partial F}{\\partial z_0}             = \\frac{\\overline{z_1}(|z_0|^2+|z_1|^2-\\overline{z_0})}{(|z_0|^2+|z_1|^2)^2}\n",
    "    2) \\frac{\\partial F}{\\partial \\overline{z_0}}  = \\frac{-z_0^2*\\overline{z_1}}{(|z_0|^2+|z_1|^2)^2}\n",
    "    3) \\frac{\\partial F}{\\partial z_1}             = \\frac{-z_0*\\overline{z_1}^2}{(|z_0|^2+|z_1|^2)^2}\n",
    "    4) \\frac{\\partial F}{\\partial \\overline{z_1}}  = \\frac{z_0(|z_0|^2+|z_1|^2-z_1)}{(|z_0|^2+|z_1|^2)^2}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Set the random seed (None for no seeding)\n",
    "seed = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Point sampling on P1\n",
    "number_of_points = int(1e3)\n",
    "\n",
    "#Homogeneous points\n",
    "if seed: np.random.seed(seed)\n",
    "sphere_points = np.random.normal(loc=0.0, scale=1.0, size=(number_of_points,4))\n",
    "sphere_points /= np.linalg.norm(sphere_points,axis=1)[:, np.newaxis]\n",
    "homogeneous_points = np.array([[pt_set[0]+1j*pt_set[1], pt_set[2]+1j*pt_set[3]] for pt_set in sphere_points])\n",
    "del(sphere_points)\n",
    "\n",
    "#Split into train, validation, test\n",
    "split_proportions = [0.7,0.1,0.2] #..note only uses train and validation proportions, assumes test as the remainder\n",
    "train_points      = homogeneous_points[:int(np.floor(number_of_points*split_proportions[0]))]\n",
    "validation_points = homogeneous_points[int(np.floor(number_of_points*split_proportions[0])):int(np.floor(number_of_points*(split_proportions[0]+split_proportions[1])))]\n",
    "test_points       = homogeneous_points[int(np.floor(number_of_points*(split_proportions[0]+split_proportions[1]))):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the ansatz for the function in homogeneous coordinates\n",
    "max_order = 1 #...select the order to extend the ansatz too\n",
    "z_0, zb_0, z_1, zb_1 = var('z_0, zb_0, z_1, zb_1', domain='complex') #...define the homogeneous variables (note define the conjugates as separate variables so can perform partial differentiation)\n",
    "\n",
    "#Initialise the function\n",
    "f(z_0,zb_0,z_1,zb_1) = 0\n",
    "#Loop through the degree orders (note following cells currently only work for max_order=1)\n",
    "for i in range(1,max_order+1):\n",
    "    #Define the coefficients for this order as variables\n",
    "    c = matrix(SR, i+1, i+1, lambda alpha,beta: var('c_{}_{}_{}'.format(i,alpha,beta), domain='real'))\n",
    "    #print('Coefficients: {c}')\n",
    "    \n",
    "    #Initialise the function contribution for this order\n",
    "    fi(z_0,z_1) = 0\n",
    "    #Loop through all homogeneous polynomials in both holomorphic & antiholomorphic parts, and add them with their respective general coefficient\n",
    "    for j1 in range(i+1):\n",
    "        for j2 in range(i+1):\n",
    "            fi += c[j1,j2] * z_0**j1 * z_1**(i-j1) * zb_0**j2 * zb_1**(i-j2) / (z_0*zb_0 + z_1*zb_1)**i\n",
    "    \n",
    "    #Add this order's ansatz to the full ansatz\n",
    "    f += fi \n",
    "    \n",
    "#Print the final ansatz form\n",
    "print(f.full_simplify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the differentiability conditions\n",
    "dFdz0(z_0,z_1)  =  (z_1*conjugate(z_1)**2)/(z_0*conjugate(z_0)+z_1*conjugate(z_1))**2\n",
    "dFdzb0(z_0,z_1) = (-z_0**2*conjugate(z_1))/(z_0*conjugate(z_0)+z_1*conjugate(z_1))**2\n",
    "dFdz1(z_0,z_1)  = (-z_0*conjugate(z_1)**2)/(z_0*conjugate(z_0)+z_1*conjugate(z_1))**2\n",
    "dFdzb1(z_0,z_1) =  (z_0**2*conjugate(z_0))/(z_0*conjugate(z_0)+z_1*conjugate(z_1))**2\n",
    "\n",
    "#Compute symbolic loss function\n",
    "LossA = norm(f.diff(z_0 ).substitute(zb_0=conjugate(z_0),zb_1=conjugate(z_1)) - dFdz0)\n",
    "LossB = norm(f.diff(zb_0).substitute(zb_0=conjugate(z_0),zb_1=conjugate(z_1)) - dFdzb0)\n",
    "LossC = norm(f.diff(z_1 ).substitute(zb_0=conjugate(z_0),zb_1=conjugate(z_1)) - dFdz1)\n",
    "LossD = norm(f.diff(zb_1).substitute(zb_0=conjugate(z_0),zb_1=conjugate(z_1)) - dFdzb1)\n",
    "Total_Loss = LossA + LossB + LossC + LossD\n",
    "Total_Loss = Total_Loss.full_simplify()\n",
    "#print(Total_Loss)\n",
    "\n",
    "#Define the loss function (evaluating for the given coefficients at the considered point)\n",
    "def Loss(constants, point):\n",
    "    return float(real_part(Total_Loss.substitute(c_1_0_0=constants[0], c_1_0_1=constants[1], c_1_1_0=constants[2], c_1_1_1=constants[3]).substitute(z_0=point[0],z_1=point[1])))\n",
    "    \n",
    "#Compute symbolic loss gradient wrt coefficients\n",
    "gradient_fn  = [Total_Loss.diff(c_1_0_0).full_simplify(),Total_Loss.diff(c_1_0_1).full_simplify(),Total_Loss.diff(c_1_1_0).full_simplify(),Total_Loss.diff(c_1_1_1).full_simplify()]\n",
    "\n",
    "#Define the loss gradient function (evaluating for the given coefficients at the considered point)\n",
    "def LossGradient(constants,point):\n",
    "    gradient_vec = [coeff_grad.substitute(c_1_0_0=constants[0], c_1_0_1=constants[1], c_1_1_0=constants[2], c_1_1_1=constants[3]) for coeff_grad in gradient_fn]\n",
    "    gradient     = np.real(np.array([coeff_grad.substitute(z_0=point[0], z_1=point[1]) for coeff_grad in gradient_vec],dtype=complex))\n",
    "    return gradient\n",
    "\n",
    "#Test\n",
    "example_coeffs, example_point = [-1,2,3,4], [3*I,5+I]\n",
    "print(f'Example coefficients {example_coeffs}, and point {example_point}')\n",
    "print(f'Loss: {Loss(example_coeffs,example_point)}')\n",
    "print(f'Loss gradient: {LossGradient(example_coeffs,example_point)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise coefficients\n",
    "if seed: np.random.seed(seed)\n",
    "coefficients = np.random.normal(loc=0.0, scale=3.0, size=4)\n",
    "print(f'Coefficients: {coefficients}\\n')\n",
    "\n",
    "#Define SGD hyperparameters\n",
    "batch_size = 32\n",
    "number_of_epochs = 10\n",
    "learning_rate = 1e-2\n",
    "momentum = 0.8\n",
    "tolerance = 1e-8\n",
    "\n",
    "#Perform the SGD\n",
    "epoch_losses = []\n",
    "for epoch in range(number_of_epochs):\n",
    "    #Shuffle the data (so batches change each epoch)\n",
    "    np.random.shuffle(train_points)\n",
    "    np.random.shuffle(validation_points)\n",
    "    \n",
    "    #Loop over batches\n",
    "    print(f'Epoch: {epoch+1}\\nProgress: ',end='')\n",
    "    batches_losses = []\n",
    "    coeff_step = 0\n",
    "    for batch_count, batch_idx in enumerate(range(np.floor(len(train_points)/batch_size).astype(int))):\n",
    "        #Compute loss for the batch\n",
    "        batches_losses.append(np.mean([Loss(coefficients,train_points[idx]) for idx in range(batch_idx*batch_size,min(number_of_points+1,(batch_idx+1)*batch_size))]))\n",
    "        #print(f'\\nBatch {batch_idx+1} Loss:\\t{batches_losses[-1]}')\n",
    "        \n",
    "        #Compute the gradient across the batch, and the respective step in the coefficients\n",
    "        grad = np.mean([LossGradient(coefficients,train_points[idx]) for idx in range(batch_idx*batch_size,min(number_of_points+1,(batch_idx+1)*batch_size))],axis=0)\n",
    "        coeff_step = - learning_rate*grad\n",
    "        #coeff_step = - ((number_of_epochs-epoch)/number_of_epochs)*learning_rate*grad #...with decay\n",
    "        #coeff_step = momentum*coeff_step - learning_rate*grad #...with momentum\n",
    "        #coeff_step = momentum*coeff_step - ((number_of_epochs-epoch)/number_of_epochs)*learning_rate*grad #...with momentum & decay\n",
    "        \n",
    "        #Early-stop the descent if the step size too small\n",
    "        if np.all(np.abs(coeff_step) <= tolerance):\n",
    "            print('...early stopping',end=' ')\n",
    "            break\n",
    "\n",
    "        #Update the coefficients\n",
    "        coefficients += coeff_step\n",
    "        if batch_count%int(len(train_points)/(20*batch_size)) == 0: print('=',end='')\n",
    "    print('x')\n",
    "    \n",
    "    #Compute epoch losses\n",
    "    train_loss = np.mean(batches_losses) \n",
    "    validation_loss = np.mean([Loss(coefficients,val_point) for val_point in validation_points])\n",
    "    print(f'Training Loss:   {train_loss}\\nValidation Loss: {validation_loss}\\nCoefficients: {coefficients}\\n')\n",
    "    epoch_losses.append([train_loss,validation_loss])\n",
    "    \n",
    "#Run testing\n",
    "epoch_losses = np.array(epoch_losses)\n",
    "test_loss = np.mean([Loss(coefficients,test_point) for test_point in test_points])\n",
    "print(f'##########\\nFinal Test Loss: {test_loss}\\nFinal Coefficients: {np.round(coefficients)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Plotting\n",
    "plt.figure()\n",
    "if number_of_epochs == 1: \n",
    "    plt.plot(range(1,len(batches_losses)+1),batches_losses,label='Batch')\n",
    "else:\n",
    "    plt.plot(range(1,len(epoch_losses)+1),epoch_losses[:,0],label='Train')\n",
    "    plt.plot(range(1,len(epoch_losses)+1),epoch_losses[:,1],label='Validation')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0)\n",
    "plt.xlim(0)\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Losses.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Potential Extensions: \n",
    "# ~ Test how performance changes with different point sampling, and number of points.\n",
    "# ~ Extend SGD (trial other learning rates, adapt decay rate (exponential) & momentum (accumulated), generalise to RMSProp & Adam).\n",
    "# ~ Extend the loss functions to higher order Ln losses (or lower with norm --> abs), add regularisation, trial other loss styles.\n",
    "# ~ Trial losses based on 2nd order derivatives (closer to Ricci scalar condition): compute by hand, then hardcode in.\n",
    "# ~ Trial other P1 functions based on the same ansatz and learn those: set the desired function's coefficients, compute derivatives by hand, hardcode in.\n",
    "# ~ Trial sage's inbuilt 'desolve' function: https://doc.sagemath.org/html/en/tutorial/tour_algebra.html#solving-differential-equations\n",
    "# ~ Generalise code usability by converting code cells to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.4",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
